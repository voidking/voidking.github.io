<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://www.voidking.com").hostname,root:"/",scheme:"Gemini",version:"7.7.1",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1,cdn:{enable:!0,url:"//qiniu-cdn.voidking.com/doc/search.xml"}},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="前言本文记录使用K8S过程中遇到的问题、解决办法和一些原理。问题排查方法参考《kubectl命令——故障排查篇》。"><meta property="og:type" content="article"><meta property="og:title" content="好好学K8S：K8S问题记录"><meta property="og:url" content="https://www.voidking.com/dev-k8s-problem/index.html"><meta property="og:site_name" content="好好学习的郝"><meta property="og:description" content="前言本文记录使用K8S过程中遇到的问题、解决办法和一些原理。问题排查方法参考《kubectl命令——故障排查篇》。"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2019-09-15T16:00:00.000Z"><meta property="article:modified_time" content="2025-02-01T08:00:00.000Z"><meta property="article:author" content="好好学习的郝"><meta property="article:tag" content="docker"><meta property="article:tag" content="k8s"><meta property="article:tag" content="问题排查"><meta property="article:tag" content="好好学K8S"><meta property="article:tag" content="etcd"><meta property="article:tag" content="ingress"><meta property="article:tag" content="kubectl"><meta property="article:tag" content="sealos"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://www.voidking.com/dev-k8s-problem/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>好好学K8S：K8S问题记录 | 好好学习的郝</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b759ac2a7fa45129e3ef060bf68259f0";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="好好学习的郝" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">好好学习的郝</span><span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">一个计算机技术爱好者与学习者</h1></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/voidking" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.voidking.com/dev-k8s-problem/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="好好学习的郝"><meta itemprop="description" content="一个计算机技术爱好者与学习者"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="好好学习的郝"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> 好好学K8S：K8S问题记录</h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-09-15 16:00:00" itemprop="dateCreated datePublished" datetime="2019-09-15T16:00:00+00:00">2019-09-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-02-01 08:00:00" itemprop="dateModified" datetime="2025-02-01T08:00:00+00:00">2025-02-01</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/" itemprop="url" rel="index"><span itemprop="name">engineering</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/troubleshooting/" itemprop="url" rel="index"><span itemprop="name">troubleshooting</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/cloudnative/" itemprop="url" rel="index"><span itemprop="name">cloudnative</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="前言"><span class="post-title-index">1.</span><a href="#前言" class="headerlink" title="前言"></a> 前言</h1><p>本文记录使用K8S过程中遇到的问题、解决办法和一些原理。<br>问题排查方法参考<a href="https://www.voidking.com/dev-kubectl-trouble-shooting/">《kubectl命令——故障排查篇》</a>。</p><span id="more"></span><h1 id="kubelet不停重启"><span class="post-title-index">2.</span><a href="#kubelet不停重启" class="headerlink" title="kubelet不停重启"></a> kubelet不停重启</h1><h2 id="问题描述"><span class="post-title-index">2.1.</span><a href="#问题描述" class="headerlink" title="问题描述"></a> 问题描述</h2><p>K8S的一个worker节点磁盘不足，关机进行磁盘扩容，物理扩容后开机，执行<code>esize2fs /dev/vdb</code>，扩容完成。</p><p><code>docker ps</code>，发现没有容器被启动。<br><code>kubectl status kubelet</code>，发现kubelet不停进行重启，每次都启动失败。<br><code>kubectl status docker</code>，正常docker正常running。</p><p>重启机器，问题依旧。</p><h2 id="排查解决"><span class="post-title-index">2.2.</span><a href="#排查解决" class="headerlink" title="排查解决"></a> 排查解决</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">journalctl -xeu kubelet -r</span><br><span class="line">journalctl -xeu docker -r</span><br></pre></td></tr></table></figure><p>kubelet日志没有报错，docker日志中报错：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">level=error msg=&quot;xxx cleanup: failed to delete container from containerd: no such container</span><br></pre></td></tr></table></figure><blockquote><p>FROM ChatGPT：<br>该错误日志表示 Docker 清理容器时失败，原因是没有找到相应的容器。</p></blockquote><blockquote><p>可能原因及解决方法：</p><ol><li>容器不存在：检查容器是否已被删除或者已经退出，如果是则不需要处理该错误。</li><li>容器正在运行：如果容器正在运行，可能是由于正在执行某些任务而无法清理。此时可以尝试停止容器后再进行清理。</li><li>Docker daemon 出现故障：在某些情况下，Docker daemon 可能会出现故障导致无法清理容器。尝试重启 Docker daemon 可能会解决问题。</li><li>操作系统出现故障：在某些情况下，操作系统可能会出现故障导致无法清理容器。尝试重启操作系统可能会解决问题。</li></ol></blockquote><p>解决办法：手动清理容器，然后重启机器。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker container prune</span><br><span class="line">reboot</span><br></pre></td></tr></table></figure><h1 id="新增节点flannel启动失败"><span class="post-title-index">3.</span><a href="#新增节点flannel启动失败" class="headerlink" title="新增节点flannel启动失败"></a> 新增节点flannel启动失败</h1><h2 id="问题描述-1"><span class="post-title-index">3.1.</span><a href="#问题描述-1" class="headerlink" title="问题描述"></a> 问题描述</h2><p>K8S集群新增了一个节点，flannel pod自动调度上去了，但是并没有启动成功。<br>查看kubelet日志，报错为：<br>[failed to find plugin “flannel” in path [/opt/cni/bin]]<br>W0523 20:49:19.343813 12586 cni.go:239] Unable to update cni config: no valid networks found in /etc/cni/net.d</p><h2 id="解决办法"><span class="post-title-index">3.2.</span><a href="#解决办法" class="headerlink" title="解决办法"></a> 解决办法</h2><p>从其他正常节点拷贝一个flannel文件到这个问题节点上的 /opt/cni/bin 目录。</p><h1 id="节点上的pod被驱逐"><span class="post-title-index">4.</span><a href="#节点上的pod被驱逐" class="headerlink" title="节点上的pod被驱逐"></a> 节点上的pod被驱逐</h1><h2 id="问题描述-2"><span class="post-title-index">4.1.</span><a href="#问题描述-2" class="headerlink" title="问题描述"></a> 问题描述</h2><p>因为磁盘压力，节点上的pod被驱逐了。但是，实际上节点还有很多磁盘空间。</p><h2 id="解决办法-1"><span class="post-title-index">4.2.</span><a href="#解决办法-1" class="headerlink" title="解决办法"></a> 解决办法</h2><p>1、查找kubelet配置文件路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status kubelet -l</span><br></pre></td></tr></table></figure><p>找到Drop-In配置文件路径，一般为：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">或者：</span><br><span class="line">/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br></pre></td></tr></table></figure><p>2、修改kubelet配置文件<br>配置文件中添加：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBELET_EXTRA_ARGS=&quot;--eviction-hard=memory.available&lt;100Mi,nodefs.available&lt;5%,imagefs.available&lt;15%,nodefs.inodesFree&lt;5%&quot;</span><br></pre></td></tr></table></figure><p>3、重启kubelet</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure><h2 id="扩展阅读"><span class="post-title-index">4.3.</span><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a> 扩展阅读</h2><p>节点压力驱逐是 kubelet 主动终止 Pod 以回收节点上资源的过程。</p><p>kubelet 监控集群节点的内存、磁盘空间和文件系统的 inode 等资源。 当这些资源中的一个或者多个达到特定的消耗水平， kubelet 可以主动地使节点上一个或者多个 Pod 失效，以回收资源防止饥饿。</p><p>在节点压力驱逐期间，kubelet 将所选 Pod 的 PodPhase 设置为 Failed。这将终止 Pod。</p><p>我们可以为 kubelet 指定自定义驱逐条件，以便在作出驱逐决定时使用。驱逐条件分为软驱逐条件和硬驱逐条件。<br>软驱逐条件将驱逐条件与管理员所必须指定的宽限期配对。 在超过宽限期之前，kubelet 不会驱逐 Pod。如果没有指定的宽限期，kubelet 会在启动时返回错误。<br>硬驱逐条件没有宽限期。当达到硬驱逐条件时， kubelet 会立即杀死 pod，而不会正常终止以回收紧缺的资源。</p><p>kubelet 具有以下默认硬驱逐条件：</p><ul><li>memory.available&lt;100Mi</li><li>nodefs.available&lt;10%</li><li>imagefs.available&lt;15%</li><li>nodefs.inodesFree&lt;5%（Linux 节点）</li></ul><p>参考文档：<a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/">节点压力驱逐</a></p><h1 id="节点上的镜像被清理"><span class="post-title-index">5.</span><a href="#节点上的镜像被清理" class="headerlink" title="节点上的镜像被清理"></a> 节点上的镜像被清理</h1><h2 id="问题描述-3"><span class="post-title-index">5.1.</span><a href="#问题描述-3" class="headerlink" title="问题描述"></a> 问题描述</h2><p>节点上的镜像，被清理掉了，但是并没有人进行过手动清理操作，也没有定时任务。<br>手动拉取镜像，过一段时间还是会被清理掉。</p><h2 id="问题原因"><span class="post-title-index">5.2.</span><a href="#问题原因" class="headerlink" title="问题原因"></a> 问题原因</h2><p>参考文档：</p><ul><li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/architecture/garbage-collection/">Garbage Collection</a></li><li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/shenyuanhaojie/p/16407553.html">K8S kubelet 资源垃圾回收机制</a></li><li><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/792212">关于Kubernetes image垃圾镜像容器的回收</a></li></ul><p>Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的所有镜像是真正意义上的所有镜像，不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限 HighThresholdPercent 时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限 LowThresholdPercent 或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄 MinAge 要求的镜像，暂不予以清理。</p><p>在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。</p><p>需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 docker run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。</p><p>影响镜像垃圾回收的关键参数有：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--image-gc-high-threshold：磁盘使用率上限，有效范围 [0-100]，默认 85</span><br><span class="line">--image-gc-low-threshold： 磁盘使用率下限，有效范围 [0-100]，默认 80</span><br><span class="line">--minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟</span><br></pre></td></tr></table></figure><p>需要注意的是，1.21 版本之后 image-gc-high-threshold 替换为了 eviction-hard，image-gc-low-threshold 替换为了 eviction-mininum-reclaim。</p><h1 id="业务服务响应很慢"><span class="post-title-index">6.</span><a href="#业务服务响应很慢" class="headerlink" title="业务服务响应很慢"></a> 业务服务响应很慢</h1><h2 id="问题描述-4"><span class="post-title-index">6.1.</span><a href="#问题描述-4" class="headerlink" title="问题描述"></a> 问题描述</h2><p>某个业务服务的Pod响应很慢，发现它的requests资源配置很低，limits资源配置很高。<br>调大requests后响应速度明显变快了，是什么原理？</p><h2 id="原理解析"><span class="post-title-index">6.2.</span><a href="#原理解析" class="headerlink" title="原理解析"></a> 原理解析</h2><p>requests是长期允许，保证资源；limits是临时允许，并不保证资源。<br>上面的问题中，因为requests配置的很低，所以只能保证requests中配置的资源，并不能保证用到用到limits中配置的资源。</p><h2 id="扩展阅读-1"><span class="post-title-index">6.3.</span><a href="#扩展阅读-1" class="headerlink" title="扩展阅读"></a> 扩展阅读</h2><p>“requests”和”limits”在Kubernetes中的原理是通过Linux的cgroups（control groups）来实现资源管理和隔离。cgroups是Linux内核提供的一种机制，它允许对进程组进行资源限制、优先级调整和统计。Kubernetes利用cgroups将资源限制和隔离应用到容器级别。</p><p>“requests”定义了容器所需的最小资源数量。Kubernetes调度器使用这个值来决定在哪个节点上运行容器，并确保节点上有足够的资源满足容器的请求。”requests”的目的是为了确保容器能够正常运行而不会遇到资源不足的问题。</p><p>而”limits”定义了容器允许使用的资源的上限。Kubernetes使用这个值来监控容器的资源使用情况，并保护节点的稳定性。如果容器试图使用超过其限制的资源量，Kubernetes会采取相应的措施，如终止容器或重新调度到其他节点。”limits”的目的是为了防止容器使用过多的资源，从而保护整个集群的稳定性。</p><p>尽管可以在容器中设置超过”requests”的资源使用量，但这并不是一个推荐的做法。当容器超过其”requests”的资源使用量时，它可能会影响其他容器的性能，导致资源竞争和不稳定的情况。超过”requests”的使用量只是暂时允许，Kubernetes会尽力满足容器的需求，但不保证持续提供额外的资源。</p><h1 id="节点资源充足但是无法调度"><span class="post-title-index">7.</span><a href="#节点资源充足但是无法调度" class="headerlink" title="节点资源充足但是无法调度"></a> 节点资源充足但是无法调度</h1><h2 id="问题描述-5"><span class="post-title-index">7.1.</span><a href="#问题描述-5" class="headerlink" title="问题描述"></a> 问题描述</h2><p>某个节点CPU、内存资源充足，也没有污点和亲和性配置，但是Pod无法调度到上面。</p><h2 id="排查解决-1"><span class="post-title-index">7.2.</span><a href="#排查解决-1" class="headerlink" title="排查解决"></a> 排查解决</h2><p>除了检查资源之外，再检查下节点上Pod数量的限制。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe node node01</span><br></pre></td></tr></table></figure><p><code>Non-terminated Pods</code> 要小于 <code>Allocatable.pods</code> ，如果Pod数量已经达到了节点上Pod数量限制，那么需要调大这个上限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim /var/lib/kubelet/config.yaml <span class="comment"># maxPods默认110，调大它</span></span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure><h1 id="kubectl-top命令执行报错"><span class="post-title-index">8.</span><a href="#kubectl-top命令执行报错" class="headerlink" title="kubectl top命令执行报错"></a> kubectl top命令执行报错</h1><h2 id="问题描述-6"><span class="post-title-index">8.1.</span><a href="#问题描述-6" class="headerlink" title="问题描述"></a> 问题描述</h2><p>执行<code>kubectl top pod</code>，报错：<br>error: Metrics API not available</p><p>在Kubernetes Dashboard上，也发现Pod的CPU使用率和内存使用都无法显示。</p><h2 id="问题分析"><span class="post-title-index">8.2.</span><a href="#问题分析" class="headerlink" title="问题分析"></a> 问题分析</h2><p>参考<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/52694238/kubectl-top-node-error-metrics-not-available-yet-using-metrics-server-as-he">kubectl top node error: metrics not available yet</a>可知，之所以出现上面的问题，是因为K8S集群没有安装 metrics-server 。</p><h2 id="解决办法-2"><span class="post-title-index">8.3.</span><a href="#解决办法-2" class="headerlink" title="解决办法"></a> 解决办法</h2><p>解决办法：安装 metrics-server 。</p><p>通用安装方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml</span><br><span class="line">kubectl apply -f components.yaml</span><br></pre></td></tr></table></figure><p>如果是sealos安装的K8S集群，那么可以使用 metrics-server 集群镜像安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sealos run labring/metrics-server:v0.6.2 --cmd=<span class="string">&quot;\</span></span><br><span class="line"><span class="string">    helm upgrade -i metrics-server charts/metrics-server \</span></span><br><span class="line"><span class="string">    -n kube-system \</span></span><br><span class="line"><span class="string">    -f /root/metrics-server/values.yaml&quot;</span></span><br></pre></td></tr></table></figure><p>其中 /root/metrics-server/values.yaml 内容为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">defaultArgs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--cert-dir=/tmp</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--kubelet-preferred-address-types=InternalIP</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--kubelet-use-node-status-port</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--metric-resolution=15s</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--kubelet-insecure-tls</span></span><br></pre></td></tr></table></figure><h1 id="kubectl-get响应慢"><span class="post-title-index">9.</span><a href="#kubectl-get响应慢" class="headerlink" title="kubectl get响应慢"></a> kubectl get响应慢</h1><h2 id="问题描述-7"><span class="post-title-index">9.1.</span><a href="#问题描述-7" class="headerlink" title="问题描述"></a> 问题描述</h2><p>执行 <code>kubectl get xxx</code> ，响应很慢，十几秒，总会几条出现提示：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">I0819 11:17:11.769943 2075052 request.go:668] Waited for 1.159893511s due to client-side throttling, not priority and fairness, request: GET:https://192.168.56.101:6443/apis/enterprisesearch.k8s.elastic.co/v1beta1?timeout=32s</span><br><span class="line">I0819 11:17:31.979933 2075052 request.go:668] Waited for 9.271822241s due to client-side throttling, not priority and fairness, request: GET:https://192.168.56.101:6443/apis/events.openfunction.io/v1alpha1?timeout=32s</span><br></pre></td></tr></table></figure><p>而且每次request后面的内容都不同。</p><h2 id="解决办法-3"><span class="post-title-index">9.2.</span><a href="#解决办法-3" class="headerlink" title="解决办法"></a> 解决办法</h2><p>解决办法：kube-apiserver 的启动参数中添加 <code>--feature-gates=APIPriorityAndFairness=false</code></p><p>具体操作方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># master节点</span></span><br><span class="line"><span class="built_in">cd</span> /etc/kubernetes/manifests</span><br><span class="line">vim kube-apiserver.yaml</span><br></pre></td></tr></table></figure><p>添加启动参数：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">kube-apiserver</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--feature-gates=APIPriorityAndFairness=false</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">...</span></span><br></pre></td></tr></table></figure><p>但是，上面的办法只能解决部分问题。<code>kubectl get pod</code>正常了，但是<code>kubectl get all</code>还是有问题。</p><p>终极解决办法：升级kubectl到v1.22版本以上。从提示其实也能看出是 client-side 侧的限流，但是 kubectl 并没有修改限流配置的方法，只能升级版本。</p><p>参考文档：</p><ul><li><a target="_blank" rel="noopener" href="https://github.com/rancher/rke2/issues/2119">Kubectl Error - Waited for … due to client-side throttling, not priority and fairness, request</a></li><li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/cluster-administration/flow-control/">API Priority and Fairness</a></li><li><a target="_blank" rel="noopener" href="https://www.infoq.cn/article/Yiqriipkfzw1gBBdyzUl">Crossplane 支持的自定义资源数量突破了 Kubernetes 的限制</a></li><li><a target="_blank" rel="noopener" href="https://www.reddit.com/r/golang/comments/wdke2u/k8s_go_client_how_to_avoid_waited_for_xs_due_to/">How to avoid “Waited for Xs due to client-side throttling, not priority and fairness”</a></li><li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/66339069/kubectl-get-all-command-return-throttling-request">kubectl get all - command return - Throttling request</a></li><li><a target="_blank" rel="noopener" href="https://www.liangzeyu.com/article/457.html">k8smaster-请求受到了节流</a></li><li><a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a></li></ul><h1 id="某个节点无法访问coredns"><span class="post-title-index">10.</span><a href="#某个节点无法访问coredns" class="headerlink" title="某个节点无法访问coredns"></a> 某个节点无法访问coredns</h1><h2 id="问题描述-8"><span class="post-title-index">10.1.</span><a href="#问题描述-8" class="headerlink" title="问题描述"></a> 问题描述</h2><p>新增了一个节点X，处于Ready状态，但是发现节点X和coredns不通。<br>具体表现为 <code>telnet 10.96.0.10 53</code> 不通，<code>nslookup www.baidu.com 10.96.0.10</code> 也无法得到解析结果。<br>与此同时，节点A是正常的，<code>telnet 10.96.0.10 53</code> 通，<code>nslookup www.baidu.com 10.96.0.10</code> 可以得到解析结果。</p><h2 id="问题排查解决"><span class="post-title-index">10.2.</span><a href="#问题排查解决" class="headerlink" title="问题排查解决"></a> 问题排查解决</h2><p>1、检查节点状态<br>所有节点都处于Ready状态。</p><p>2、检查pod网络组件<br>查看节点X的flannel和kube-proxy，都运行正常。</p><p>3、确认问题边界</p><ul><li>节点X ping 节点A和节点B，通</li><li>节点X telnet 另外的 service，不通</li><li>节点X ping coredns pod ip，不通</li><li>节点X ping 本机 pod ip，通</li><li>节点X ping 节点A pod ip，通</li><li>节点X ping 节点B pod ip，不通</li></ul><p>这就有意思了，节点X和主机A pod通，和主机B pod不通，大概率是CNI的问题。</p><p>4、检查所有主机的cni插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get nodes</span><br><span class="line">kubectl get pod -n kube-system -owide | grep flannel</span><br></pre></td></tr></table></figure><p>10个节点，发现只有5个节点有flannel，但是这些没有flannel的节点也处于Ready状态，也是个bug。</p><p>继续排查发现没有flannel的节点存在污点，因此给flannel添加容忍，使每个节点都部署上CNI插件。</p><p>至此，问题解决。</p><h2 id="根因分析"><span class="post-title-index">10.3.</span><a href="#根因分析" class="headerlink" title="根因分析"></a> 根因分析</h2><p>CNI插件的根本作用是调整节点上网络的转发规则。<br>虽然节点X上的CNI插件正常，但是有5个节点缺少CNI插件，那么当节点X上调整网络转发规则时，会漏掉这5个节点上的pod，也就是说节点X上没有网络转发规则可以访问到它们。</p><h1 id="某个节点上的pod全部pending"><span class="post-title-index">11.</span><a href="#某个节点上的pod全部pending" class="headerlink" title="某个节点上的pod全部pending"></a> 某个节点上的pod全部pending</h1><h2 id="问题描述-9"><span class="post-title-index">11.1.</span><a href="#问题描述-9" class="headerlink" title="问题描述"></a> 问题描述</h2><p>节点X处于Ready状态，pod也可以调度到上面，但是所有调度到上面的pod全部处于pending状态，也没有任何events输出。</p><h2 id="解决办法-4"><span class="post-title-index">11.2.</span><a href="#解决办法-4" class="headerlink" title="解决办法"></a> 解决办法</h2><p>解决办法：重启节点X的kubelet。</p><h1 id="read-eth0-speed-invalid-argument"><span class="post-title-index">12.</span><a href="#read-eth0-speed-invalid-argument" class="headerlink" title="read eth0/speed: invalid argument"></a> read eth0/speed: invalid argument</h1><h2 id="问题描述-10"><span class="post-title-index">12.1.</span><a href="#问题描述-10" class="headerlink" title="问题描述"></a> 问题描述</h2><p>个别GPU节点上，运行 gpu-feature-discovery 报错：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">E0927 10:43:40.329641       1 network.go:146] failed to read net iface attribute speed: read /host-sys/class/net/eth0/speed: invalid argument</span><br></pre></td></tr></table></figure><p>登录GPU节点，执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /host-sys/class/net/eth0/speed</span><br></pre></td></tr></table></figure><p>同样会报错 invalid argument</p><h2 id="解决办法-5"><span class="post-title-index">12.2.</span><a href="#解决办法-5" class="headerlink" title="解决办法"></a> 解决办法</h2><p>升级内核。</p><p>参考文档：</p><ul><li><a target="_blank" rel="noopener" href="https://unix.stackexchange.com/questions/353911/cant-read-a-specific-file-under-sys-filesystem">Can’t read a specific file under /sys filesystem</a></li></ul><h1 id="无法删除pod情况一"><span class="post-title-index">13.</span><a href="#无法删除pod情况一" class="headerlink" title="无法删除pod情况一"></a> 无法删除pod情况一</h1><h2 id="问题描述-11"><span class="post-title-index">13.1.</span><a href="#问题描述-11" class="headerlink" title="问题描述"></a> 问题描述</h2><p>一些Pod处于Terminating状态，但是一直无法被删除。<br>使用<code>--force</code>和<code>--grace-period</code>强制删除，会报出警告：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.</span><br></pre></td></tr></table></figure><h2 id="解决办法-6"><span class="post-title-index">13.2.</span><a href="#解决办法-6" class="headerlink" title="解决办法"></a> 解决办法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod xxx -n yyy</span><br></pre></td></tr></table></figure><p>可以看到具体的pod无法被删除的原因，比如：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Warning  Unhealthy  12s (x3182 over 8h)  kubelet  Readiness probe errored: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</span><br></pre></td></tr></table></figure><p>根据提示检查docker状态，发现docker没有正常运行，启动它即可。</p><h1 id="无法删除pod情况二"><span class="post-title-index">14.</span><a href="#无法删除pod情况二" class="headerlink" title="无法删除pod情况二"></a> 无法删除pod情况二</h1><h2 id="问题描述-12"><span class="post-title-index">14.1.</span><a href="#问题描述-12" class="headerlink" title="问题描述"></a> 问题描述</h2><p>一些Pod处于Terminating状态，但是一直无法被删除。<br>使用<code>--force</code>和<code>--grace-period</code>强制删除，依然会卡住，无法删除。</p><h2 id="解决办法-7"><span class="post-title-index">14.2.</span><a href="#解决办法-7" class="headerlink" title="解决办法"></a> 解决办法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod xxx -n yyy</span><br></pre></td></tr></table></figure><p>依然看不到无法删除pod的原因，只能看到Status为Terminating，State为Terminated，Reason为Error。</p><p>解决办法：编辑pod，删除 metadata.finalizers 字段。</p><p>参考文档：<a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/finalizers/">Finalizers</a></p><h1 id="无法删除CRD"><span class="post-title-index">15.</span><a href="#无法删除CRD" class="headerlink" title="无法删除CRD"></a> 无法删除CRD</h1><h2 id="问题描述-13"><span class="post-title-index">15.1.</span><a href="#问题描述-13" class="headerlink" title="问题描述"></a> 问题描述</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete crd ingresses.networking.internal.knative.dev</span><br></pre></td></tr></table></figure><p>卸载knative时，删除CRD ingresses.networking.internal.knative.dev ，无法删除。</p><h2 id="解决办法-8"><span class="post-title-index">15.2.</span><a href="#解决办法-8" class="headerlink" title="解决办法"></a> 解决办法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit crd ingresses.networking.internal.knative.dev</span><br></pre></td></tr></table></figure><p>删除其中的 metadata.finalizers 字段。</p><p>参考文档：<a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/finalizers/">Finalizers</a></p><h1 id="日志不存在"><span class="post-title-index">16.</span><a href="#日志不存在" class="headerlink" title="日志不存在"></a> 日志不存在</h1><h2 id="问题描述-14"><span class="post-title-index">16.1.</span><a href="#问题描述-14" class="headerlink" title="问题描述"></a> 问题描述</h2><p><code>systemctl status kubelet</code>，发现大量报错：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">11月 28 15:48:31 node1 kubelet[1663]: E1122 15:48:31.502542    1663 cri_stats_provider.go:669] &quot;Unable to fetch container log stats&quot; err=&quot;failed to get fsstats for \&quot;/var/log/pods/calico-system_csi-node-driver-jbf9l_969616b6-784a-4154-8161-24206f4501cc/calico-csi/21.log\&quot;: no such file or directory&quot; containerName=&quot;calico-csi&quot;</span><br><span class="line">11月 28 15:48:31 node1 kubelet[1663]: E1122 15:48:31.502610    1663 cri_stats_provider.go:669] &quot;Unable to fetch container log stats&quot; err=&quot;failed to get fsstats for \&quot;/var/log/pods/kube-system_kube-proxy-9vh2d_f52465f9-229f-4895-9c23-3366b9d71c11/kube-proxy/21.log\&quot;: no such file or directory&quot; containerName=&quot;kube-proxy&quot;</span><br><span class="line">11月 28 15:48:31 node1 kubelet[1663]: E1122 15:48:31.502645    1663 cri_stats_provider.go:669] &quot;Unable to fetch container log stats&quot; err=&quot;failed to get fsstats for \&quot;/var/log/pods/calico-system_calico-node-68w9g_d33768de-9b24-43f6-ba60-ec9917889847/calico-node/0.log\&quot;: no such file or directory&quot; containerName=&quot;calico-node&quot;</span><br></pre></td></tr></table></figure><h2 id="问题排查解决-1"><span class="post-title-index">16.2.</span><a href="#问题排查解决-1" class="headerlink" title="问题排查解决"></a> 问题排查解决</h2><p>查看报错中的日志路径：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/var/log/pods/calico-system_csi-node-driver-jbf9l_969616b6-784a-4154-8161-24206f4501cc/calico-csi/21.log -&gt; /var/lib/docker/containers/8b352f0c0582b0c048449fef8305fde15ce4017dbb37abfaedcd698e9e1d2ffb/8b352f0c0582b0c048449fef8305fde15ce4017dbb37abfaedcd698e9e1d2ffb-json.log</span><br></pre></td></tr></table></figure><p>日志路径是一个软链，而真实日志已经不存在了。</p><p>解决办法为删除所有失效软链：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">find /var/log/pods -<span class="built_in">type</span> l ! -<span class="built_in">exec</span> <span class="built_in">test</span> -e &#123;&#125; \; -<span class="built_in">print</span></span><br><span class="line"></span><br><span class="line">find /var/log/pods -<span class="built_in">type</span> l ! -<span class="built_in">exec</span> <span class="built_in">test</span> -e &#123;&#125; \; -delete</span><br></pre></td></tr></table></figure><p>然后重启kubelet</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart kubelet</span><br><span class="line">systemctl status kubelet</span><br></pre></td></tr></table></figure><h1 id="流量经过ingress偶发502问题"><span class="post-title-index">17.</span><a href="#流量经过ingress偶发502问题" class="headerlink" title="流量经过ingress偶发502问题"></a> 流量经过ingress偶发502问题</h1><h2 id="问题描述-15"><span class="post-title-index">17.1.</span><a href="#问题描述-15" class="headerlink" title="问题描述"></a> 问题描述</h2><p>100个并发压测一个接口，直接压测service，全部200；通过域名压测时，无论是在内网压测还是外网压测，都会出现1%的502。</p><p>服务pod日志中，没有502。<br>ingress access.log 中，出现502。<br>ingress errer.log 中，出现104。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2023/12/09 02:35:50 [error] 7960#7960: *60866823 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 172.20.0.100, server: test.voidking.com, request: &quot;POST /api/v1/user/login HTTP/1.1&quot;, upstream: &quot;http://100.107.80.144:80/api/v1/user/login&quot;, host: &quot;test.voidking.com&quot;</span><br><span class="line">2023/12/09 02:36:05 [error] 7373#7373: *60866826 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 172.20.0.100, server: test.voidking.com, request: &quot;POST /api/v1/user/login HTTP/1.1&quot;, upstream: &quot;http://100.82.122.191:80/api/v1/user/login&quot;, host: &quot;test.voidking.com&quot;</span><br></pre></td></tr></table></figure><h2 id="问题排查解决-2"><span class="post-title-index">17.2.</span><a href="#问题排查解决-2" class="headerlink" title="问题排查解决"></a> 问题排查解决</h2><p>1、调整服务pod资源、ingress pod资源、coredns pod资源，都没有效果。<br>2、去掉tls证书，没有效果。<br>3、调整ingress内核参数，没有效果。<br>4、调整ingress <code>upstream-keepalive-timeout</code> 参数为 4 ，502数量降低到了 0.1% ，效果明显！调整 ingress <code>upstream-keepalive-timeout</code> 参数为 30，502数量降低到了 0.1% 。</p><p>注意：要在ingress controller的configMap中修改 <code>upstream-keepalive-timeout</code> 配置，而不是ingress的annotations（经验证无效）。</p><p>参考文档：</p><ul><li><a target="_blank" rel="noopener" href="https://www.modb.pro/db/405641">如何在 Kubernetes 中进行 ingress-nginx 配置优化以及HTTP请求速率限制</a></li><li><a target="_blank" rel="noopener" href="https://blog.csdn.net/aexwx/article/details/126180053">我们是如何解决偶发性的 502 错误的</a></li></ul><h2 id="扩展阅读-2"><span class="post-title-index">17.3.</span><a href="#扩展阅读-2" class="headerlink" title="扩展阅读"></a> 扩展阅读</h2><p>502错误是网关错误，504错误是网关超时。<br>有个比较好的比喻：502是电话打通没人接，504是电话打不通。</p><p>什么是keepalive？HTTP/1.1中的Keep-Alive是一种机制，它允许在单个TCP连接上发送多个HTTP请求和响应，而不需要为每个请求/响应对都建立一个新的连接。在HTTP/1.0中，每个HTTP请求都需要建立一个新的TCP连接，这会导致一些性能上的开销。HTTP/1.1引入了Keep-Alive以解决这个问题。</p><p>那么 keepalive_timout 和 502 错误之间有什么关系？因为网站的架构不是浏览器直接连接后端的应用服务器，而是中间有nginx服务器做反向代理，浏览器和nginx服务器之间建立keepalive连接，nginx再和后端的应用服务器建立keepalive连接，所以这是两种不同的keepalive连接。</p><p>我们把浏览器和nginx之间的keepalive连接叫做ka1，把nginx和应用服务器之间的keepalive连接叫做ka2。如果ka1的超时设置为100秒，也就是说如果100秒之内没有新内容要传输，就把nginx和浏览器之间的连接断掉。而同时，我们把ka2设置为50秒，也就是说如果nginx和应用服务器之间没有新内容要传输，那么就把应用服务器和nginx之间的连接断掉。那么这时候就会产生一个问题：前50秒没有传输内容，在第51秒的时候，浏览器向nginx发了一个请求，这时候ka1还没有断掉，因为没有到100秒的时间，所以这是没有问题的，但是当nginx试图向应用服务器发请求的时候就出问题了，ka2断了！因为ka2的超时设置是50秒，这时候已经超了，所以就断了，这时候nginx无法再从应用服务器获得正确响应，只好返回浏览器502错误！</p><p>因此，解决办法也就明确了：让ka1小于ka2。</p><h1 id="udp包问题"><span class="post-title-index">18.</span><a href="#udp包问题" class="headerlink" title="udp包问题"></a> udp包问题</h1><h2 id="问题描述-16"><span class="post-title-index">18.1.</span><a href="#问题描述-16" class="headerlink" title="问题描述"></a> 问题描述</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffprobe rtsp://user:password@192.168.56.200:8080/api/to/stream</span><br></pre></td></tr></table></figure><p>在宿主机上执行上面这条命令，可以正常获取到视频流。<br>启动一个容器，执行上面这条命令，可以正常获取到视频流。<br>使用同一个镜像启动Pod，执行上面这条命令，获取到视频流时会卡住。</p><p>正常输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ffprobe version 4.2.7-0ubuntu0.1 Copyright (c) 2007-2022 the FFmpeg developers</span><br><span class="line">  ...</span><br><span class="line">  libavutil      56. 31.100 / 56. 31.100</span><br><span class="line">  libavcodec     58. 54.100 / 58. 54.100</span><br><span class="line">  libavformat    58. 29.100 / 58. 29.100</span><br><span class="line">  libavdevice    58.  8.100 / 58.  8.100</span><br><span class="line">  libavfilter     7. 57.100 /  7. 57.100</span><br><span class="line">  libavresample   4.  0.  0 /  4.  0.  0</span><br><span class="line">  libswscale      5.  5.100 /  5.  5.100</span><br><span class="line">  libswresample   3.  5.100 /  3.  5.100</span><br><span class="line">  libpostproc    55.  5.100 / 55.  5.100</span><br><span class="line">Input #0, rtsp, from &#x27;rtsp://user:password@192.168.56.200:8080/api/to/stream&#x27;:</span><br><span class="line">  Metadata:</span><br><span class="line">    title           : Media Presentation</span><br><span class="line">  Duration: N/A, start: 0.000000, bitrate: N/A</span><br><span class="line">    Stream #0:0: Video: hevc (Main), yuv420p(tv), 1280x720, 32 fps, 25 tbr, 90k tbn, 32 tbc</span><br><span class="line">    Stream #0:1: Audio: pcm_alaw, 8000 Hz, 1 channels, s16, 64 kb/s</span><br></pre></td></tr></table></figure><p>卡住的输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ffprobe version 4.2.7-0ubuntu0.1 Copyright (c) 2007-2022 the FFmpeg developers</span><br><span class="line">  ...</span><br><span class="line">  libavutil      56. 31.100 / 56. 31.100</span><br><span class="line">  libavcodec     58. 54.100 / 58. 54.100</span><br><span class="line">  libavformat    58. 29.100 / 58. 29.100</span><br><span class="line">  libavdevice    58.  8.100 / 58.  8.100</span><br></pre></td></tr></table></figure><h2 id="解决办法-9"><span class="post-title-index">18.2.</span><a href="#解决办法-9" class="headerlink" title="解决办法"></a> 解决办法</h2><p>指定 tcp 可以正常获取到视频流。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffprobe rtsp://user:password@192.168.56.200:8080/api/to/stream -rtsp_transport tcp</span><br></pre></td></tr></table></figure><p>从问题表现，只能推测出calico网络插件对于udp包的处理存在问题，具体原因还需要探索。</p><h1 id="卸载istio后无法创建pod问题"><span class="post-title-index">19.</span><a href="#卸载istio后无法创建pod问题" class="headerlink" title="卸载istio后无法创建pod问题"></a> 卸载istio后无法创建pod问题</h1><h2 id="问题描述-17"><span class="post-title-index">19.1.</span><a href="#问题描述-17" class="headerlink" title="问题描述"></a> 问题描述</h2><p>卸载istio后，创建了一个deployment，但是k8s无法创建出这个deployment对应的pod，到不了调度阶段，看不到pod。</p><p>查看deployment的描述，发现报错：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Warning  FailedCreate      24m (x17 over 29m)    deployment-controller  Error creating: Internal error occurred: failed calling webhook &quot;rev.object.sidecar-injector.istio.io&quot;: Post &quot;https://istiod.istio-system.svc:443/inject?timeout=10s&quot;: no endpoints available for service &quot;istiod&quot;</span><br></pre></td></tr></table></figure><h2 id="原因分析"><span class="post-title-index">19.2.</span><a href="#原因分析" class="headerlink" title="原因分析"></a> 原因分析</h2><p>出现这个问题的原因是卸载 Istio 后，ValidatingWebhookConfiguration 还保留在 Kubernetes 系统中。当 Kubernetes API server 内部试图调用 webhook 但无法找到相应的 webhook 服务端点时，会出现此错误。</p><h2 id="解决办法-10"><span class="post-title-index">19.3.</span><a href="#解决办法-10" class="headerlink" title="解决办法"></a> 解决办法</h2><p>1、列出k8s集群中所有的 MutatingWebhookConfiguration 和 ValidatingWebhookConfiguration</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get mutatingwebhookconfigurations</span><br><span class="line">kubectl get validatingwebhookconfigurations</span><br></pre></td></tr></table></figure><p>找到与 Istio 相关的 webhook 配置。</p><p>2、删除与 Istio 相关的 webhook 配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete mutatingwebhookconfigurations &lt;your-mutating-webhook-config-name&gt;</span><br><span class="line">kubectl delete validatingwebhookconfigurations &lt;your-validating-webhook-config-name&gt;</span><br></pre></td></tr></table></figure><p>删除这些 webhook 配置之后，deployment 对应的 pods 就能够正常创建了。</p><h1 id="pod网段、service网段与docker网段冲突问题"><span class="post-title-index">20.</span><a href="#pod网段、service网段与docker网段冲突问题" class="headerlink" title="pod网段、service网段与docker网段冲突问题"></a> pod网段、service网段与docker网段冲突问题</h1><h2 id="问题描述-18"><span class="post-title-index">20.1.</span><a href="#问题描述-18" class="headerlink" title="问题描述"></a> 问题描述</h2><p>部署k8s时规划网段不合理，给pod或service规划的网段为 172.17.0.0/16 ，刚好和docker的默认网段 172.17.0.0/16 冲突了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ip addr show docker0</span><br><span class="line">kubectl describe pod -n kube-system $(kubectl get pods -n kube-system | grep kube-apiserver | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>) | grep service-cluster-ip-range</span><br><span class="line">kubectl get ippool -oyaml | grep cidr</span><br></pre></td></tr></table></figure><h2 id="解决办法-11"><span class="post-title-index">20.2.</span><a href="#解决办法-11" class="headerlink" title="解决办法"></a> 解决办法</h2><p>如果刚开始部署k8s集群就发现这个问题，那么给pod或service修改一个网段，重新部署即可。</p><p>如果已经使用了一段时间后才发现这个问题，那么修改起来就比较麻烦了。这里有两个解决办法：<br>方法一：修改docker默认网段（验证可行）</p><p>1、停止kubelet和docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop kubelet</span><br><span class="line">systemctl stop docker</span><br></pre></td></tr></table></figure><p>2、编辑docker配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/docker/daemon.json</span><br></pre></td></tr></table></figure><p>添加一行docker网关ip配置：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;bip&quot;: &quot;192.168.254.1/24&quot;,</span><br></pre></td></tr></table></figure><p>3、启动docker和kubelet</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start docker</span><br><span class="line">systemctl start kubelet</span><br></pre></td></tr></table></figure><p>4、查看修改后的docker0 IP</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip addr show docker0</span><br></pre></td></tr></table></figure><p>方法二：修改pod或service的网段（未验证）<br>参考文档：</p><ul><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/598561146">k8s更换svc和pod网段</a></li><li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/60176343/how-to-make-the-pod-cidr-range-larger-in-kubernetes-cluster-deployed-with-kubead">How to make the pod CIDR range larger in kubernetes cluster deployed with kubeadm?</a></li><li><a target="_blank" rel="noopener" href="https://docs.tigera.io/calico/latest/networking/ipam/migrate-pools">Migrate from one IP pool to another</a></li></ul><h1 id="怎样扩展service网段？"><span class="post-title-index">21.</span><a href="#怎样扩展service网段？" class="headerlink" title="怎样扩展service网段？"></a> 怎样扩展service网段？</h1><h2 id="问题描述-19"><span class="post-title-index">21.1.</span><a href="#问题描述-19" class="headerlink" title="问题描述"></a> 问题描述</h2><p>发现knative controller报错：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">failed to create private K8s Service: Internal error occurred: failed to allocate a serviceIP: range is full</span><br></pre></td></tr></table></figure><h2 id="解决办法-12"><span class="post-title-index">21.2.</span><a href="#解决办法-12" class="headerlink" title="解决办法"></a> 解决办法</h2><p>1、确认serviceIP是否已经用完</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod -n kube-system $(kubectl get pods -n kube-system | grep kube-apiserver | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>) | grep service-cluster-ip-range</span><br><span class="line">kubectl get svc -A | <span class="built_in">wc</span> -l</span><br></pre></td></tr></table></figure><p>如果发现 <code>--service-cluster-ip-range=10.96.0.0/22</code> ，那么最多可以创建 1022 个 serviceIP。当实际svc数量大于1022时，那么证明serviceIP确实用完了。</p><p>2、修改apiserver配置，扩展service网段<br>在master节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/kubernetes/manifests/kube-apiserver.yaml</span><br></pre></td></tr></table></figure><p>修改 serviceIP 范围为 <code>10.96.0.0/20</code></p><p>3、检查 kube-apiserver 是否重启（配置发生修改时会自动重启）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n kube-system | grep kube-apiserver</span><br></pre></td></tr></table></figure><p>4、验证修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod -n kube-system $(kubectl get pods -n kube-system | grep kube-apiserver | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>) | grep service-cluster-ip-range</span><br></pre></td></tr></table></figure><h1 id="怎样修改Service-NodePort范围？"><span class="post-title-index">22.</span><a href="#怎样修改Service-NodePort范围？" class="headerlink" title="怎样修改Service NodePort范围？"></a> 怎样修改Service NodePort范围？</h1><h2 id="问题描述-20"><span class="post-title-index">22.1.</span><a href="#问题描述-20" class="headerlink" title="问题描述"></a> 问题描述</h2><p>在Kubernetes中，默认情况下，NodePort类型的Service可以使用的端口范围是30000-32767。<br>在某些情况下，我们可能需要使用更大的端口范围，此时该怎样修改Service NodePort的端口范围？</p><h2 id="修改方法"><span class="post-title-index">22.2.</span><a href="#修改方法" class="headerlink" title="修改方法"></a> 修改方法</h2><p>1、登录master节点，修改 apiserver 配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/kubernetes/manifests/kube-apiserver.yaml</span><br></pre></td></tr></table></figure><p>添加 <code>--service-node-port-range</code> 参数：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-apiserver</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">...</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--service-node-port-range=10000-60000</span></span><br></pre></td></tr></table></figure><p>修改后，会自动重建 apiserver pod<br>注意：最小端口建议大于1024，这样可以避免占用系统保留端口。</p><p>2、查看新的 apiserver pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure><h1 id="pvc内容未更新问题"><span class="post-title-index">23.</span><a href="#pvc内容未更新问题" class="headerlink" title="pvc内容未更新问题"></a> pvc内容未更新问题</h1><h2 id="问题描述-21"><span class="post-title-index">23.1.</span><a href="#问题描述-21" class="headerlink" title="问题描述"></a> 问题描述</h2><p>已知条件：</p><ul><li>多个pod挂载了同一个pvc</li><li>pvc后端基于cephfs，挂载路径为 /data -&gt; cephfs:/path/to/data</li><li>cephfs挂载到了一台主机A，挂载路径为 /path -&gt; cephfs:/path</li></ul><p>问题：在主机A上更新了 /path/to/data 中的内容后，从pod中看，/data 中的内容并没有更新；而新创建的pod，/data 中的内容是有更新的。</p><h2 id="问题排查解决-3"><span class="post-title-index">23.2.</span><a href="#问题排查解决-3" class="headerlink" title="问题排查解决"></a> 问题排查解决</h2><p>同步情况验证：<br>1、更新主机A /path/to/data 中的内容，发现老的pod /data 中的内容不会更新，新的pod /data 中的内容会更新。<br>2、更新新的pod /data 中的内容，发现老的pod /data 中的内容不会更新，主机A /path/to/data 中的内容会更新。<br>3、更新老的pod /data 中的内容，发现新的pod /data 中的内容不会更新，主机A /path/to/data 中的内容不会更新，但是其他老的pod /data 中的内容会更新。</p><p>以上问题表现，有两个怀疑：1）K8S某些节点有问题，没有处理好cephfs同步问题；2）cephfs存在问题，多副本间没有实现一致性。<br>但是，检查了K8S节点，没有发现问题；检查cephfs日志，也没有发现问题。</p><p>意外发现：老的 pod /data 中内容更新后会同步到主机A上的 /path/to/data.bak 目录！</p><p>由此问题就清晰了：有位同学在主机A上更新 /path/to/data 中的内容时，并不是直接修改该目录中的内容，而是直接把该目录重命名为了 /path/to/data.bak，然后重建了一个目录 /path/to/data ，最终导致内容同步出现问题。<br>这是因为cephfs挂载时读取文件名，会转换成一个统一标志符再进行挂载，哪怕重命名了，挂载的依然是原本的目录，所以老的pod挂载的目录，从 /path/to/data 变成了 /path/to/data.bak ，新的pod挂载的目录是 /path/to/data</p><h1 id="pod中执行bash速度很慢"><span class="post-title-index">24.</span><a href="#pod中执行bash速度很慢" class="headerlink" title="pod中执行bash速度很慢"></a> pod中执行bash速度很慢</h1><h2 id="问题描述-22"><span class="post-title-index">24.1.</span><a href="#问题描述-22" class="headerlink" title="问题描述"></a> 问题描述</h2><p>在pod中，执行 <code>bash</code> 会卡住，要10s左右才能执行成功，切到新的bash shell。</p><h2 id="问题排查解决-4"><span class="post-title-index">24.2.</span><a href="#问题排查解决-4" class="headerlink" title="问题排查解决"></a> 问题排查解决</h2><p>1、更换pod镜像，问题依然存在。证明不是镜像的问题。<br>2、更换Docker版本，升级内核版本，更换操作系统，问题依然存在。证明不是它们的问题。<br>3、执行bash时抓包，并没有网络流量。证明不是因为访问网络耗时导致的速度慢。<br>4、执行 bash 时速度慢，执行 sh 时速度很快。证明执行速度慢和命令有关。<br>5、删除 .profile、.bashrc 等文件，执行 bash 速度快了一些，大概需要 5s 左右。证明 bash 速度慢和 bash 初始化有关。<br>6、执行 env 发现 pod 中有上万条环境变量，unset 这些环境变量后执行 bash 不再卡住。证明执行 bash 卡住的根本原因是 pod 环境变量过多。</p><p>问题原因：k8s中，默认情况下，同一个 namespace 的 service 信息会作为环境变量注入到 pod 中。<br>解决办法：pod 中不要默认注入 service 信息。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">enableServiceLinks:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">my-image</span></span><br></pre></td></tr></table></figure><p>此外，同一个namespace下的service过多，还可能引起pod无法启动，报错：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">standard_init_linux.go:211: exec user process caused &quot;argument list too long&quot;</span><br></pre></td></tr></table></figure><p>解决办法：pod 中不要默认注入 service 信息。参考文档：<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/issues/84539">standard_init_linux.go:211: exec user process caused argument list too long</a></p><h1 id="服务不可用说明"><span class="post-title-index">25.</span><a href="#服务不可用说明" class="headerlink" title="服务不可用说明"></a> 服务不可用说明</h1><h2 id="问题描述-23"><span class="post-title-index">25.1.</span><a href="#问题描述-23" class="headerlink" title="问题描述"></a> 问题描述</h2><p>当服务A不可用时，怎样给服务A添加一个不可用说明？</p><h2 id="解决办法-13"><span class="post-title-index">25.2.</span><a href="#解决办法-13" class="headerlink" title="解决办法"></a> 解决办法</h2><p>创建一个说明服务B，当服务A不可用时，调整服务A的ingress配置，流量打到服务B。<br>说明服务B定义参考：<a target="_blank" rel="noopener" href="https://github.com/voidking/hexo-storage/blob/main/k8s-problem/xxx-unavailable.yaml">xxx-unavailable.yaml</a></p><h1 id="修改Ingress配置不生效问题"><span class="post-title-index">26.</span><a href="#修改Ingress配置不生效问题" class="headerlink" title="修改Ingress配置不生效问题"></a> 修改Ingress配置不生效问题</h1><h2 id="问题描述-24"><span class="post-title-index">26.1.</span><a href="#问题描述-24" class="headerlink" title="问题描述"></a> 问题描述</h2><p>修改了一个 ingress 配置，但是测试发现没有生效。<br>查看ingress-nginx日志，发现报错：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Failed to determine a suitable ClusterIP Endpoint for Service &quot;default/demo&quot;: service &quot;default/demo&quot; does not exist</span><br></pre></td></tr></table></figure><h2 id="问题排查解决-5"><span class="post-title-index">26.2.</span><a href="#问题排查解决-5" class="headerlink" title="问题排查解决"></a> 问题排查解决</h2><p>问题原因：这个报错的原因是 ingress 中配置使用了 default namespace 中的 demo service，但是这个service已经不存在了。</p><p>解决办法：找到使用 demo service 的 ingress，修改或删除它</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get ingress -n default -oyaml | grep demo -A10</span><br></pre></td></tr></table></figure><h1 id="etcd-报错-database-space-exceeded"><span class="post-title-index">27.</span><a href="#etcd-报错-database-space-exceeded" class="headerlink" title="etcd 报错 database space exceeded"></a> etcd 报错 database space exceeded</h1><h2 id="问题描述-25"><span class="post-title-index">27.1.</span><a href="#问题描述-25" class="headerlink" title="问题描述"></a> 问题描述</h2><p>执行 kubectl 命令时报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error from server: etcdserver: mvcc: database space exceeded</span><br></pre></td></tr></table></figure><h2 id="解决办法-14"><span class="post-title-index">27.2.</span><a href="#解决办法-14" class="headerlink" title="解决办法"></a> 解决办法</h2><p>参考文档：<a target="_blank" rel="noopener" href="https://discuss.kubernetes.io/t/etcdserver-mvcc-database-space-exceeded/22806/3">Etcdserver: mvcc: database space exceeded</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">ETCDCTL_API=3 etcdctl \</span><br><span class="line">--endpoints=https://[127.0.0.1]:2379 \</span><br><span class="line">--cacert=/etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert=/etc/kubernetes/pki/etcd/server.crt \</span><br><span class="line">--key=/etc/kubernetes/pki/etcd/server.key \</span><br><span class="line">endpoint status --write-out=<span class="string">&quot;json&quot;</span> | egrep -o <span class="string">&#x27;&quot;revision&quot;:[0-9]*&#x27;</span> | egrep -o <span class="string">&#x27;[0-9].*&#x27;</span></span><br><span class="line"></span><br><span class="line">ETCDCTL_API=3 etcdctl \</span><br><span class="line">--endpoints=https://[127.0.0.1]:2379 \</span><br><span class="line">--cacert=/etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert=/etc/kubernetes/pki/etcd/server.crt \</span><br><span class="line">--key=/etc/kubernetes/pki/etcd/server.key \</span><br><span class="line">compact 359226442</span><br><span class="line"></span><br><span class="line">ETCDCTL_API=3 etcdctl \</span><br><span class="line">--endpoints=https://[127.0.0.1]:2379 \</span><br><span class="line">--cacert=/etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--cert=/etc/kubernetes/pki/etcd/server.crt \</span><br><span class="line">--key=/etc/kubernetes/pki/etcd/server.key \</span><br><span class="line">compact defrag --cluster</span><br></pre></td></tr></table></figure></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> 好好学习的郝</li><li class="post-copyright-link"> <strong>原文链接：</strong> <a href="https://www.voidking.com/dev-k8s-problem/" title="好好学K8S：K8S问题记录">https://www.voidking.com/dev-k8s-problem/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本文采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议，转载请注明出处！源站会即时更新知识点并修正错误，欢迎访问~</li><li> <img width="200" height="200" src="/images/avatar.jpg"><p style="text-align:center;margin-bottom:0">微信公众号同步更新，欢迎关注~</p></li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/docker/" rel="tag"># docker</a> <a href="/tags/k8s/" rel="tag"># k8s</a> <a href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag"># 问题排查</a> <a href="/tags/%E5%A5%BD%E5%A5%BD%E5%AD%A6K8S/" rel="tag"># 好好学K8S</a> <a href="/tags/etcd/" rel="tag"># etcd</a> <a href="/tags/ingress/" rel="tag"># ingress</a> <a href="/tags/kubectl/" rel="tag"># kubectl</a> <a href="/tags/sealos/" rel="tag"># sealos</a></div><div class="post-nav"><div class="post-nav-item"><a href="/dev-k8s-schedule/" rel="prev" title="好好学K8S：K8S中的调度"><i class="fa fa-chevron-left"></i> 好好学K8S：K8S中的调度</a></div><div class="post-nav-item"> <a href="/dev-k8s-trouble-shooting/" rel="next" title="好好学K8S：K8S故障排查手册">好好学K8S：K8S故障排查手册<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="footer-ads" style="margin-top:10px"><ins class="adsbygoogle" style="display:block" data-ad-format="autorelaxed" data-ad-client="ca-pub-3284447971731414" data-ad-slot="9697986181"></ins><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3284447971731414" crossorigin="anonymous"></script><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-text">1. 前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#kubelet%E4%B8%8D%E5%81%9C%E9%87%8D%E5%90%AF"><span class="nav-text">2. kubelet不停重启</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0"><span class="nav-text">2.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%92%E6%9F%A5%E8%A7%A3%E5%86%B3"><span class="nav-text">2.2. 排查解决</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%B0%E5%A2%9E%E8%8A%82%E7%82%B9flannel%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5"><span class="nav-text">3. 新增节点flannel启动失败</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-1"><span class="nav-text">3.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="nav-text">3.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84pod%E8%A2%AB%E9%A9%B1%E9%80%90"><span class="nav-text">4. 节点上的pod被驱逐</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-2"><span class="nav-text">4.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-1"><span class="nav-text">4.2. 解决办法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB"><span class="nav-text">4.3. 扩展阅读</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84%E9%95%9C%E5%83%8F%E8%A2%AB%E6%B8%85%E7%90%86"><span class="nav-text">5. 节点上的镜像被清理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-3"><span class="nav-text">5.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%8E%9F%E5%9B%A0"><span class="nav-text">5.2. 问题原因</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%9A%E5%8A%A1%E6%9C%8D%E5%8A%A1%E5%93%8D%E5%BA%94%E5%BE%88%E6%85%A2"><span class="nav-text">6. 业务服务响应很慢</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-4"><span class="nav-text">6.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90"><span class="nav-text">6.2. 原理解析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB-1"><span class="nav-text">6.3. 扩展阅读</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E8%B5%84%E6%BA%90%E5%85%85%E8%B6%B3%E4%BD%86%E6%98%AF%E6%97%A0%E6%B3%95%E8%B0%83%E5%BA%A6"><span class="nav-text">7. 节点资源充足但是无法调度</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-5"><span class="nav-text">7.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%92%E6%9F%A5%E8%A7%A3%E5%86%B3-1"><span class="nav-text">7.2. 排查解决</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#kubectl-top%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E6%8A%A5%E9%94%99"><span class="nav-text">8. kubectl top命令执行报错</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-6"><span class="nav-text">8.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90"><span class="nav-text">8.2. 问题分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-2"><span class="nav-text">8.3. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#kubectl-get%E5%93%8D%E5%BA%94%E6%85%A2"><span class="nav-text">9. kubectl get响应慢</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-7"><span class="nav-text">9.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-3"><span class="nav-text">9.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9F%90%E4%B8%AA%E8%8A%82%E7%82%B9%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AEcoredns"><span class="nav-text">10. 某个节点无法访问coredns</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-8"><span class="nav-text">10.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%A7%A3%E5%86%B3"><span class="nav-text">10.2. 问题排查解决</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E5%9B%A0%E5%88%86%E6%9E%90"><span class="nav-text">10.3. 根因分析</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9F%90%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84pod%E5%85%A8%E9%83%A8pending"><span class="nav-text">11. 某个节点上的pod全部pending</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-9"><span class="nav-text">11.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-4"><span class="nav-text">11.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#read-eth0-speed-invalid-argument"><span class="nav-text">12. read eth0&#x2F;speed: invalid argument</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-10"><span class="nav-text">12.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-5"><span class="nav-text">12.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4pod%E6%83%85%E5%86%B5%E4%B8%80"><span class="nav-text">13. 无法删除pod情况一</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-11"><span class="nav-text">13.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-6"><span class="nav-text">13.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4pod%E6%83%85%E5%86%B5%E4%BA%8C"><span class="nav-text">14. 无法删除pod情况二</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-12"><span class="nav-text">14.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-7"><span class="nav-text">14.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4CRD"><span class="nav-text">15. 无法删除CRD</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-13"><span class="nav-text">15.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-8"><span class="nav-text">15.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%A5%E5%BF%97%E4%B8%8D%E5%AD%98%E5%9C%A8"><span class="nav-text">16. 日志不存在</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-14"><span class="nav-text">16.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%A7%A3%E5%86%B3-1"><span class="nav-text">16.2. 问题排查解决</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B5%81%E9%87%8F%E7%BB%8F%E8%BF%87ingress%E5%81%B6%E5%8F%91502%E9%97%AE%E9%A2%98"><span class="nav-text">17. 流量经过ingress偶发502问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-15"><span class="nav-text">17.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%A7%A3%E5%86%B3-2"><span class="nav-text">17.2. 问题排查解决</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB-2"><span class="nav-text">17.3. 扩展阅读</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#udp%E5%8C%85%E9%97%AE%E9%A2%98"><span class="nav-text">18. udp包问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-16"><span class="nav-text">18.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-9"><span class="nav-text">18.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%B8%E8%BD%BDistio%E5%90%8E%E6%97%A0%E6%B3%95%E5%88%9B%E5%BB%BApod%E9%97%AE%E9%A2%98"><span class="nav-text">19. 卸载istio后无法创建pod问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-17"><span class="nav-text">19.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90"><span class="nav-text">19.2. 原因分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-10"><span class="nav-text">19.3. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pod%E7%BD%91%E6%AE%B5%E3%80%81service%E7%BD%91%E6%AE%B5%E4%B8%8Edocker%E7%BD%91%E6%AE%B5%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98"><span class="nav-text">20. pod网段、service网段与docker网段冲突问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-18"><span class="nav-text">20.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-11"><span class="nav-text">20.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%8E%E6%A0%B7%E6%89%A9%E5%B1%95service%E7%BD%91%E6%AE%B5%EF%BC%9F"><span class="nav-text">21. 怎样扩展service网段？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-19"><span class="nav-text">21.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-12"><span class="nav-text">21.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%8E%E6%A0%B7%E4%BF%AE%E6%94%B9Service-NodePort%E8%8C%83%E5%9B%B4%EF%BC%9F"><span class="nav-text">22. 怎样修改Service NodePort范围？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-20"><span class="nav-text">22.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E6%96%B9%E6%B3%95"><span class="nav-text">22.2. 修改方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pvc%E5%86%85%E5%AE%B9%E6%9C%AA%E6%9B%B4%E6%96%B0%E9%97%AE%E9%A2%98"><span class="nav-text">23. pvc内容未更新问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-21"><span class="nav-text">23.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%A7%A3%E5%86%B3-3"><span class="nav-text">23.2. 问题排查解决</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pod%E4%B8%AD%E6%89%A7%E8%A1%8Cbash%E9%80%9F%E5%BA%A6%E5%BE%88%E6%85%A2"><span class="nav-text">24. pod中执行bash速度很慢</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-22"><span class="nav-text">24.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%A7%A3%E5%86%B3-4"><span class="nav-text">24.2. 问题排查解决</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E4%B8%8D%E5%8F%AF%E7%94%A8%E8%AF%B4%E6%98%8E"><span class="nav-text">25. 服务不可用说明</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-23"><span class="nav-text">25.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-13"><span class="nav-text">25.2. 解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9Ingress%E9%85%8D%E7%BD%AE%E4%B8%8D%E7%94%9F%E6%95%88%E9%97%AE%E9%A2%98"><span class="nav-text">26. 修改Ingress配置不生效问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-24"><span class="nav-text">26.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%A7%A3%E5%86%B3-5"><span class="nav-text">26.2. 问题排查解决</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#etcd-%E6%8A%A5%E9%94%99-database-space-exceeded"><span class="nav-text">27. etcd 报错 database space exceeded</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0-25"><span class="nav-text">27.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-14"><span class="nav-text">27.2. 解决办法</span></a></li></ol></li></ol></div></div><div class="sidecar-ads" style="margin-top:10px"><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="好好学习的郝" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">好好学习的郝</p><div class="site-description" itemprop="description">一个计算机技术爱好者与学习者</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">748</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">32</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">259</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="mailto:voidking@qq.com" title="E-Mail → mailto:voidking@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://github.com/voidking" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;voidking" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="http://weibo.com/voidking" title="Weibo → http:&#x2F;&#x2F;weibo.com&#x2F;voidking" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/voidking" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;voidking" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i> Zhihu</a></span></div></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2014 – <span itemprop="copyrightYear">2025</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">好好学习的郝</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div><div class="footer-beian"> <a href="http://beian.miit.gov.cn/" target="_blank">苏ICP备14021030号</a>&nbsp;|&nbsp; <img src="/images/beian.png" alt=""> <a target="_blank" href="http://www.beian.gov.cn/">苏公网安备 32032202000223号</a></div></div></footer></div><script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/gitalk/1.7.2/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/gitalk/1.7.2/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '5a238b8c32b1e4dd2156',
      clientSecret: 'bfb5d518626f6fdc7da0351d1e0cd37ab75c6361',
      repo: 'gitalk-comments',
      owner: 'voidking',
      admin: ['voidking'],
      id: 'd4a92bbb4c3a00c5faf3a51031b6448f',
      title: '好好学K8S：K8S问题记录',
      body: '欢迎留言，互相交流学习~',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></body></html>