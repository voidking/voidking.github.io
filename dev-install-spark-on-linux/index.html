<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://www.voidking.com").hostname,root:"/",scheme:"Gemini",version:"7.7.1",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1,cdn:{enable:!0,url:"//qiniu.cdn.voidking.com/doc/search.xml"}},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="前言计划在三台Linux主机中搭建Spark 3.3.1集群，主机配置为4C8G，操作系统为CentOS7。三台主机的IP为： 123192.168.56.101192.168.56.102192.168.56.103 选择101作为master节点，另外两个作为worker节点。 参考文档：  spark3.3.0安装&amp;部署过程 Spark Overview Spark Standalo"><meta property="og:type" content="article"><meta property="og:title" content="Linux中搭建Spark集群"><meta property="og:url" content="https://www.voidking.com/dev-install-spark-on-linux/index.html"><meta property="og:site_name" content="好好学习的郝"><meta property="og:description" content="前言计划在三台Linux主机中搭建Spark 3.3.1集群，主机配置为4C8G，操作系统为CentOS7。三台主机的IP为： 123192.168.56.101192.168.56.102192.168.56.103 选择101作为master节点，另外两个作为worker节点。 参考文档：  spark3.3.0安装&amp;部署过程 Spark Overview Spark Standalo"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2022-09-12T20:00:00.000Z"><meta property="article:modified_time" content="2022-11-15T09:00:00.000Z"><meta property="article:author" content="好好学习的郝"><meta property="article:tag" content="python"><meta property="article:tag" content="问题排查"><meta property="article:tag" content="java"><meta property="article:tag" content="maven"><meta property="article:tag" content="spark"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://www.voidking.com/dev-install-spark-on-linux/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>Linux中搭建Spark集群 | 好好学习的郝</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b759ac2a7fa45129e3ef060bf68259f0";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="好好学习的郝" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">好好学习的郝</span><span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">一个计算机技术爱好者与学习者</h1></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/voidking" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.voidking.com/dev-install-spark-on-linux/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="好好学习的郝"><meta itemprop="description" content="一个计算机技术爱好者与学习者"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="好好学习的郝"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> Linux中搭建Spark集群</h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-09-12 20:00:00" itemprop="dateCreated datePublished" datetime="2022-09-12T20:00:00+00:00">2022-09-12</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-11-15 09:00:00" itemprop="dateModified" datetime="2022-11-15T09:00:00+00:00">2022-11-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/" itemprop="url" rel="index"><span itemprop="name">engineering</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/troubleshooting/" itemprop="url" rel="index"><span itemprop="name">troubleshooting</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="前言"><span class="post-title-index">1.</span><a href="#前言" class="headerlink" title="前言"></a> 前言</h1><p>计划在三台Linux主机中搭建Spark 3.3.1集群，主机配置为4C8G，操作系统为CentOS7。<br>三台主机的IP为：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">192.168.56.101</span></span><br><span class="line"><span class="number">192.168.56.102</span></span><br><span class="line"><span class="number">192.168.56.103</span></span><br></pre></td></tr></table></figure><p>选择101作为master节点，另外两个作为worker节点。</p><p>参考文档：</p><ul><li><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_61022929/article/details/126403124">spark3.3.0安装&amp;部署过程</a></li><li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/">Spark Overview</a></li><li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li><li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/submitting-applications.html">Submitting Applications</a></li></ul><span id="more"></span><h1 id="安装Java"><span class="post-title-index">2.</span><a href="#安装Java" class="headerlink" title="安装Java"></a> 安装Java</h1><p>参考文档<a href="https://www.voidking.com/dev-install-jdk-on-all-platforms/">《全平台安装JDK》</a></p><h1 id="安装Spark"><span class="post-title-index">3.</span><a href="#安装Spark" class="headerlink" title="安装Spark"></a> 安装Spark</h1><h2 id="master节点配置"><span class="post-title-index">3.1.</span><a href="#master节点配置" class="headerlink" title="master节点配置"></a> master节点配置</h2><p>1、下载spark并解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3-scala2.13.tgz</span><br><span class="line"><span class="built_in">mkdir</span> -p /usr/local/spark</span><br><span class="line">tar -xzvf spark-3.3.1-bin-hadoop3-scala2.13.tgz -C /usr/local/spark</span><br></pre></td></tr></table></figure><p>这里选择spark-3.3.1-bin-hadoop3-scala2.13.tgz版本，带着scala一起。<br>spark-3.3.1-bin-hadoop3.tgz也包含scala，但是版本好像有些问题，因此不选择它。</p><p>更多版本的spark，可以在 <a target="_blank" rel="noopener" href="https://spark.apache.org/downloads.html">Download Apache Spark</a> 和 <a target="_blank" rel="noopener" href="https://archive.apache.org/dist/spark/">Spark release archives</a> 页面找到。</p><p>2、创建配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark/spark-3.3.1-bin-hadoop3-scala2.13/conf</span><br><span class="line"><span class="built_in">cp</span> workers.template workers</span><br><span class="line"><span class="built_in">cp</span> spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"><span class="built_in">cp</span> spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p>3、修改配置<br>（1）workers中删除localhost，添加</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#192.168.56.101</span><br><span class="line">192.168.56.102</span><br><span class="line">192.168.56.103</span><br></pre></td></tr></table></figure><p>（2）spark-defaults.conf暂时不变</p><p>（3）spark-env.sh中添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.8.0_161</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=192.168.56.101</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure><p>4、/etc/profile中添加环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/local/spark/spark-3.3.1-bin-hadoop3-scala2.13</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><p>5、使配置生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h2 id="master配置同步到worker"><span class="post-title-index">3.2.</span><a href="#master配置同步到worker" class="headerlink" title="master配置同步到worker"></a> master配置同步到worker</h2><p>1、打包spark程序和配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark</span><br><span class="line">tar -czvf spark-3.3.1-bin-hadoop3-scala2.13.tgz spark-3.3.1-bin-hadoop3-scala2.13</span><br></pre></td></tr></table></figure><p>2、拷贝程序和配置到worker节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scp spark-3.3.1-bin-hadoop3-scala2.13.tgz 192.168.56.102:~</span><br><span class="line">scp /etc/profile 192.168.56.102:/etc/profile</span><br><span class="line"></span><br><span class="line">scp spark-3.3.1-bin-hadoop3-scala2.13.tgz 192.168.56.103:~</span><br><span class="line">scp /etc/profile 192.168.56.103:/etc/profile</span><br></pre></td></tr></table></figure><h2 id="worker节点配置"><span class="post-title-index">3.3.</span><a href="#worker节点配置" class="headerlink" title="worker节点配置"></a> worker节点配置</h2><p>1、解压spark</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /usr/local/spark</span><br><span class="line">tar -xzvf spark-3.3.1-bin-hadoop3-scala2.13.tgz -C /usr/local/spark</span><br></pre></td></tr></table></figure><h2 id="运行Spark"><span class="post-title-index">3.4.</span><a href="#运行Spark" class="headerlink" title="运行Spark"></a> 运行Spark</h2><p>1、启动spark（在master节点上执行）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark/spark-3.3.1-bin-hadoop3-scala2.13/sbin</span><br><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><p>根据提示，依次输入两台worker节点的密码。（这里最好配置上免密登录）<br>这样，三个节点上的spark就都可以启动起来。</p><p>2、验证安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>master节点看到Master和Jps两个进程，worker节点看到Worker和Jps两个进程。</p><h1 id="使用Spark"><span class="post-title-index">4.</span><a href="#使用Spark" class="headerlink" title="使用Spark"></a> 使用Spark</h1><h2 id="单机测试"><span class="post-title-index">4.1.</span><a href="#单机测试" class="headerlink" title="单机测试"></a> 单机测试</h2><p>1、在master或者worker节点上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark/spark-3.3.1-bin-hadoop3-scala2.13/</span><br><span class="line"></span><br><span class="line">./bin/run-example SparkPi 10</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">22/11/09 20:38:29 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 1.118 s</span><br><span class="line">22/11/09 20:38:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job</span><br><span class="line">22/11/09 20:38:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished</span><br><span class="line">22/11/09 20:38:29 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 1.175116 s</span><br><span class="line">Pi is roughly 3.138907138907139</span><br><span class="line">22/11/09 20:38:29 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040</span><br></pre></td></tr></table></figure><p>2、查看执行进度<br>浏览器访问 <a target="_blank" rel="noopener" href="http://192.168.56.101:8080/">http://192.168.56.101:8080</a></p><h2 id="spark-submit提交任务"><span class="post-title-index">4.2.</span><a href="#spark-submit提交任务" class="headerlink" title="spark-submit提交任务"></a> spark-submit提交任务</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://192.168.56.101:7077 \</span><br><span class="line">  ./examples/jars/spark-examples_2.12-3.3.1.jar 1000</span><br></pre></td></tr></table></figure><p>spark-submit提交的任务，执行速度非常非常慢，不知道为什么，留个坑吧。。。</p><h2 id="连接mysql"><span class="post-title-index">4.3.</span><a href="#连接mysql" class="headerlink" title="连接mysql"></a> 连接mysql</h2><p>1、创建测试文件 test_mysql.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_sql</span>():</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;testmysql&#x27;</span>).master(<span class="string">&#x27;local&#x27;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://192.168.56.101:3306/testdb&quot;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;org.mariadb.jdbc.Driver&quot;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;testtable&quot;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;xxxxxx&quot;</span>)\</span><br><span class="line">        .load()</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.select(df[<span class="string">&#x27;testcolumn&#x27;</span>]).show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    test_sql()</span><br></pre></td></tr></table></figure><p>2、提交测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit mysql.py</span><br></pre></td></tr></table></figure><h2 id="连接kafka"><span class="post-title-index">4.4.</span><a href="#连接kafka" class="headerlink" title="连接kafka"></a> 连接kafka</h2><p>1、创建测试文件 test_kafka.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_data</span>():</span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;testkafka&#x27;</span>).getOrCreate()</span><br><span class="line">    <span class="comment"># Creating a Kafka Source for Streaming Queries</span></span><br><span class="line">    df = spark.readStream\</span><br><span class="line">        .<span class="built_in">format</span>(<span class="string">&#x27;kafka&#x27;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.56.101:9092&quot;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;subscribe&quot;</span>, <span class="string">&quot;customer_volume&quot;</span>)\</span><br><span class="line">        .load()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&gt;&gt;&gt;&gt; print schema:&quot;</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.selectExpr(<span class="string">&quot;CAST(key AS STRING)&quot;</span>, <span class="string">&quot;CAST(value AS STRING)&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    read_data()</span><br></pre></td></tr></table></figure><p>2、提交测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit test_kafka.py</span><br></pre></td></tr></table></figure><dl><dt>报错：<br>py4j.protocol.Py4JJavaError: An error occurred while calling o29.load.</dt><dd>java.lang.NoClassDefFoundError: org/apache/kafka/common/serialization/ByteArraySerializer</dd></dl><p>看提示是因为缺少kafka序列化类，根本原因是因为缺少kafka client jar包。</p><p>解决办法，下载<a target="_blank" rel="noopener" href="https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients/3.3.1">kafka-clients jar</a>，放入到 <code>/usr/local/spark/spark-3.3.1-bin-hadoop3-scala2.13/jars</code> 目录中</p><p>类似的，spark连接kafka相关的jar包还有：</p><ul><li><a target="_blank" rel="noopener" href="https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10_2.13/3.3.1">spark-sql-kafka jar</a></li><li><a target="_blank" rel="noopener" href="https://mvnrepository.com/artifact/org.apache.spark/spark-token-provider-kafka-0-10_2.13/3.3.1">spark-token-provider-kafka jar</a></li><li><a target="_blank" rel="noopener" href="https://mvnrepository.com/artifact/org.apache.commons/commons-pool2/2.11.1">commons-pool2 jar</a></li></ul><p>参考文档：<a target="_blank" rel="noopener" href="https://blog.csdn.net/dkl12/article/details/118637084">Spark 3.0.1 Structured Streaming 提交程序异常解决</a></p><h1 id="后记"><span class="post-title-index">5.</span><a href="#后记" class="headerlink" title="后记"></a> 后记</h1><p>对于缺少jar包问题，都可以到 <a target="_blank" rel="noopener" href="https://mvnrepository.com/">MVNREPOSITORY</a> 这个网站查找需要的jar包，很全。</p></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> 好好学习的郝</li><li class="post-copyright-link"> <strong>原文链接：</strong> <a href="https://www.voidking.com/dev-install-spark-on-linux/" title="Linux中搭建Spark集群">https://www.voidking.com/dev-install-spark-on-linux/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本文采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议，转载请注明出处！源站会即时更新知识点并修正错误，欢迎访问~</li><li> <img width="200" height="200" src="/images/avatar.jpg"><p style="text-align:center;margin-bottom:0">微信公众号同步更新，欢迎关注~</p></li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag"># 问题排查</a> <a href="/tags/java/" rel="tag"># java</a> <a href="/tags/maven/" rel="tag"># maven</a> <a href="/tags/spark/" rel="tag"># spark</a></div><div class="post-nav"><div class="post-nav-item"><a href="/dev-spark-start/" rel="prev" title="Spark入门篇"><i class="fa fa-chevron-left"></i> Spark入门篇</a></div><div class="post-nav-item"> <a href="/dev-linux-network-proxy/" rel="next" title="好好学Linux：Linux配置网络代理">好好学Linux：Linux配置网络代理<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="footer-ads" style="margin-top:10px"><ins class="adsbygoogle" style="display:block" data-ad-format="autorelaxed" data-ad-client="ca-pub-3284447971731414" data-ad-slot="9697986181"></ins><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3284447971731414" crossorigin="anonymous"></script><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-text">1. 前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85Java"><span class="nav-text">2. 安装Java</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85Spark"><span class="nav-text">3. 安装Spark</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#master%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE"><span class="nav-text">3.1. master节点配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#master%E9%85%8D%E7%BD%AE%E5%90%8C%E6%AD%A5%E5%88%B0worker"><span class="nav-text">3.2. master配置同步到worker</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#worker%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE"><span class="nav-text">3.3. worker节点配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%90%E8%A1%8CSpark"><span class="nav-text">3.4. 运行Spark</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Spark"><span class="nav-text">4. 使用Spark</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E6%9C%BA%E6%B5%8B%E8%AF%95"><span class="nav-text">4.1. 单机测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark-submit%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1"><span class="nav-text">4.2. spark-submit提交任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5mysql"><span class="nav-text">4.3. 连接mysql</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5kafka"><span class="nav-text">4.4. 连接kafka</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%90%8E%E8%AE%B0"><span class="nav-text">5. 后记</span></a></li></ol></div></div><div class="sidecar-ads" style="margin-top:10px"><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="好好学习的郝" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">好好学习的郝</p><div class="site-description" itemprop="description">一个计算机技术爱好者与学习者</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">723</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">31</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">254</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="mailto:voidking@qq.com" title="E-Mail → mailto:voidking@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://github.com/voidking" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;voidking" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="http://weibo.com/voidking" title="Weibo → http:&#x2F;&#x2F;weibo.com&#x2F;voidking" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/voidking" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;voidking" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i> Zhihu</a></span></div></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2014 – <span itemprop="copyrightYear">2024</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">好好学习的郝</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div><div class="footer-beian"> <a href="http://beian.miit.gov.cn/" target="_blank">苏ICP备14021030号</a>&nbsp;|&nbsp; <img src="/images/beian.png" alt=""> <a target="_blank" href="http://www.beian.gov.cn/">苏公网安备 32032202000223号</a></div></div></footer></div><script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/gitalk/1.7.2/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/gitalk/1.7.2/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '5a238b8c32b1e4dd2156',
      clientSecret: 'bfb5d518626f6fdc7da0351d1e0cd37ab75c6361',
      repo: 'gitalk-comments',
      owner: 'voidking',
      admin: ['voidking'],
      id: 'a2bfe80e8b50b211c2bdc6e77c7d8026',
      title: 'Linux中搭建Spark集群',
      body: '欢迎留言，互相交流学习~',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></body></html>