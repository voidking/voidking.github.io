<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://www.voidking.com").hostname,root:"/",scheme:"Gemini",version:"7.7.1",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1,cdn:{enable:!0,url:"//cdn.voidking.com/search.xml"}},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="前言计划在三台Linux主机中搭建Spark on Yarn集群，Spark版本3.2.3，Hadoop版本3.2.3。 主机配置为4C8G，操作系统为CentOS7，hosts配置为： 123192.168.56.101 spark-master192.168.56.102 spark-slave1192.168.56.103 spark-slave2 选择101作为master节点，另外两个作"><meta property="og:type" content="article"><meta property="og:title" content="搭建Spark on Yarn集群"><meta property="og:url" content="https://www.voidking.com/dev-spark-on-yarn/index.html"><meta property="og:site_name" content="好好学习的郝"><meta property="og:description" content="前言计划在三台Linux主机中搭建Spark on Yarn集群，Spark版本3.2.3，Hadoop版本3.2.3。 主机配置为4C8G，操作系统为CentOS7，hosts配置为： 123192.168.56.101 spark-master192.168.56.102 spark-slave1192.168.56.103 spark-slave2 选择101作为master节点，另外两个作"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-05-14T08:00:00.000Z"><meta property="article:modified_time" content="2023-05-14T08:00:00.000Z"><meta property="article:author" content="好好学习的郝"><meta property="article:tag" content="问题排查"><meta property="article:tag" content="java"><meta property="article:tag" content="hadoop"><meta property="article:tag" content="spark"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://www.voidking.com/dev-spark-on-yarn/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>搭建Spark on Yarn集群 | 好好学习的郝</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b759ac2a7fa45129e3ef060bf68259f0";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="好好学习的郝" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">好好学习的郝</span><span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">一个计算机技术爱好者与学习者</h1></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/voidking" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.voidking.com/dev-spark-on-yarn/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="好好学习的郝"><meta itemprop="description" content="一个计算机技术爱好者与学习者"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="好好学习的郝"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> 搭建Spark on Yarn集群</h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-05-14 08:00:00" itemprop="dateCreated datePublished" datetime="2023-05-14T08:00:00+00:00">2023-05-14</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/" itemprop="url" rel="index"><span itemprop="name">engineering</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/engineering/troubleshooting/" itemprop="url" rel="index"><span itemprop="name">troubleshooting</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="前言"><span class="post-title-index">1.</span><a href="#前言" class="headerlink" title="前言"></a> 前言</h1><p>计划在三台Linux主机中搭建Spark on Yarn集群，Spark版本3.2.3，Hadoop版本3.2.3。</p><p>主机配置为4C8G，操作系统为CentOS7，hosts配置为：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">192.168.56.101</span> spark-master</span><br><span class="line"><span class="number">192.168.56.102</span> spark-slave1</span><br><span class="line"><span class="number">192.168.56.103</span> spark-slave2</span><br></pre></td></tr></table></figure><p>选择101作为master节点，另外两个作为worker节点。</p><p>参考文档：</p><ul><li><a href="https://www.voidking.com/dev-install-hadoop-on-linux/">《Linux中安装配置Hadoop》</a></li><li><a href="https://www.voidking.com/dev-install-spark-on-linux/">《Linux中搭建Spark集群》</a></li><li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011374856/article/details/119869602">Hadoop 3.2.2 安装与使用文档超详细图文步骤</a></li><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/487572725">spark3.2+hadoop3.3.2三节点分布式部署初体验</a></li><li><a target="_blank" rel="noopener" href="https://juejin.cn/post/7095277036868436004">搭建Spark on Yarn集群</a></li><li><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_61022929/article/details/126403124">spark3.3.0安装&amp;部署过程</a></li></ul><span id="more"></span><h1 id="安装Java"><span class="post-title-index">2.</span><a href="#安装Java" class="headerlink" title="安装Java"></a> 安装Java</h1><p>参考文档<a href="https://www.voidking.com/dev-install-jdk-on-all-platforms/">《全平台安装JDK》</a></p><h1 id="安装Hadoop"><span class="post-title-index">3.</span><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a> 安装Hadoop</h1><h2 id="下载Hadoop安装包"><span class="post-title-index">3.1.</span><a href="#下载Hadoop安装包" class="headerlink" title="下载Hadoop安装包"></a> 下载Hadoop安装包</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz --no-check-certificate</span><br><span class="line"><span class="built_in">mkdir</span> -p /usr/local/hadoop/</span><br><span class="line">tar -xzvf hadoop-3.2.3.tar.gz -C /usr/local/hadoop/</span><br></pre></td></tr></table></figure><h2 id="修改Hadoop配置"><span class="post-title-index">3.2.</span><a href="#修改Hadoop配置" class="headerlink" title="修改Hadoop配置"></a> 修改Hadoop配置</h2><p>1、修改hadoop-env.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/hadoop/hadoop-3.2.3</span><br><span class="line">vim etc/hadoop/hadoop-env.sh</span><br></pre></td></tr></table></figure><p>修改JAVA_HOME为绝对路径。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.8.0_161</span><br></pre></td></tr></table></figure><p>2、验证环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop version</span><br></pre></td></tr></table></figure><h2 id="单机测试运行"><span class="post-title-index">3.3.</span><a href="#单机测试运行" class="headerlink" title="单机测试运行"></a> 单机测试运行</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> ./input</span><br><span class="line"><span class="built_in">cp</span> ./etc/hadoop/*.xml ./input</span><br><span class="line">./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output <span class="string">&#x27;dfs[a-z.]+&#x27;</span></span><br><span class="line"><span class="built_in">cat</span> ./output/*</span><br></pre></td></tr></table></figure><p>看到结果<code>1 dfsadmin</code>表明运行成功。</p><h2 id="master节点配置HDFS"><span class="post-title-index">3.4.</span><a href="#master节点配置HDFS" class="headerlink" title="master节点配置HDFS"></a> master节点配置HDFS</h2><p>1、修改etc/hadoop/core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hdfs中nomenode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://spark-master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/hadoop-3.2.3/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置参考文档：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-common/core-default.xml">hadoop3.2.3 core-default.xml</a></p><p>2、修改etc/hadoop/hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置dfs副本数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- hdfs的web管理页面的端口 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.http.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 设置secondnamenode的端口 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark-master:9001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- namenode目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/hadoop-3.2.3/data/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- datanode目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/hadoop-3.2.3/data/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置参考文档：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hadoop3.2.3 hdfs-default.xml</a></p><p>3、修改etc/hadoop/workers</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-master</span><br><span class="line">spark-slave1</span><br><span class="line">spark-slave2</span><br></pre></td></tr></table></figure><h2 id="master节点配置配置Yarn"><span class="post-title-index">3.5.</span><a href="#master节点配置配置Yarn" class="headerlink" title="master节点配置配置Yarn"></a> master节点配置配置Yarn</h2><p>1、编辑 etc/hadoop/mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定mr运行在yarn上 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>2、编辑 etc/hadoop/yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- reducer 获取数据的方式 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 YARN 的 ResourceManager 的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark-master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!-- 该节点上YARN可使用的物理内存总量，默认是 8192（MB）--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 注意，如果你的节点内存资源不够8GB，则需要调减小这个值 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!-- 单个任务可申请最少内存，默认 1024 MB --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">&lt;!-- 单个任务可申请最大内存，默认 8192 MB --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="配置环境变量"><span class="post-title-index">3.6.</span><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a> 配置环境变量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><p>添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/local/hadoop/hadoop-3.2.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JRE_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><p>使生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h2 id="拷贝配置到slave节点"><span class="post-title-index">3.7.</span><a href="#拷贝配置到slave节点" class="headerlink" title="拷贝配置到slave节点"></a> 拷贝配置到slave节点</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rsync -P -avz /usr/local/hadoop 192.168.56.102:/usr/local/</span><br><span class="line">rsync -P -avz /usr/local/hadoop 192.168.56.103:/usr/local/</span><br><span class="line">rsync -P -avz /etc/profile 192.168.56.102:/etc/profile</span><br><span class="line">rsync -P -avz /etc/profile 192.168.56.103:/etc/profile</span><br></pre></td></tr></table></figure><h2 id="master节点启动HDFS"><span class="post-title-index">3.8.</span><a href="#master节点启动HDFS" class="headerlink" title="master节点启动HDFS"></a> master节点启动HDFS</h2><p>1、格式化namenode</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>执行完成，没有报错，当前目录中出现了tmp目录，表明格式化成功。slave节点不会出现tmp目录。</p><p>2、添加HDFS用户环境变量<br>/etc/profile中添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_SECURE_USER=hdfs</span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure><p>或者sbin/start-dfs.sh 和 sbin/stop-dfs.sh，文件顶部添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HDFS_DATANODE_USER=root</span><br><span class="line">HDFS_DATANODE_SECURE_USER=hdfs</span><br><span class="line">HDFS_NAMENODE_USER=root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure><p>否则执行可能报错：<br>Starting namenodes on [spark-master]<br>ERROR: Attempting to operate on hdfs namenode as root<br>ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.<br>Starting datanodes<br>ERROR: Attempting to operate on hdfs datanode as root<br>ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.<br>Starting secondary namenodes [spark-master]<br>ERROR: Attempting to operate on hdfs secondarynamenode as root<br>ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.</p><p>3、启动hdfs服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>这条命令会在master和slave节点同时启动hdfs，会提示输入密码，最好提前配置好免密登录。</p><p>4、查看hdfs进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps -l</span><br></pre></td></tr></table></figure><p>master节点可以看到：</p><ul><li>org.apache.hadoop.hdfs.server.namenode.NameNode</li><li>org.apache.hadoop.hdfs.server.datanode.DataNode</li><li>org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode</li></ul><p>slave节点可以看到：</p><ul><li>org.apache.hadoop.hdfs.server.datanode.DataNode</li></ul><p>5、浏览器访问<br>浏览器访问 <a target="_blank" rel="noopener" href="http://192.168.56.101:50070/">http://192.168.56.101:50070</a><br>可以看到 NameNode 和 Datanode 的信息。</p><h2 id="master节点启动Yarn"><span class="post-title-index">3.9.</span><a href="#master节点启动Yarn" class="headerlink" title="master节点启动Yarn"></a> master节点启动Yarn</h2><p>1、修改 sbin/start-yarn.sh 和 sbin/stop-yarn.sh，文件顶部添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">HADOOP_SECURE_DN_USER=yarn</span><br><span class="line">YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><p>2、启动yarn</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>3、查看进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps -l</span><br></pre></td></tr></table></figure><p>master可以看到：</p><ul><li>org.apache.hadoop.yarn.server.nodemanager.NodeManager</li><li>org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</li></ul><p>slave节点可以看到：</p><ul><li>org.apache.hadoop.yarn.server.nodemanager.NodeManager</li></ul><p>4、浏览器访问<br>浏览器访问 <a target="_blank" rel="noopener" href="http://192.168.56.101:8088/">http://192.168.56.101:8088</a><br>可以看到 Yarn ResourceManager的信息。</p><h1 id="安装Spark"><span class="post-title-index">4.</span><a href="#安装Spark" class="headerlink" title="安装Spark"></a> 安装Spark</h1><h2 id="master节点配置"><span class="post-title-index">4.1.</span><a href="#master节点配置" class="headerlink" title="master节点配置"></a> master节点配置</h2><p>1、下载spark并解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2-scala2.13.tgz</span><br><span class="line"><span class="built_in">mkdir</span> -p /usr/local/spark</span><br><span class="line">tar -xzvf spark-3.2.3-bin-hadoop3.2-scala2.13.tgz -C /usr/local/spark</span><br></pre></td></tr></table></figure><p>更多版本的spark，可以在<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/spark/">Spark release archives</a>页面找到。</p><p>2、创建配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark/spark-3.2.3-bin-hadoop3.2-scala2.13/conf</span><br><span class="line"><span class="built_in">cp</span> workers.template workers</span><br><span class="line"><span class="built_in">cp</span> spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"><span class="built_in">cp</span> spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p>3、修改配置<br>（1）workers中删除localhost，添加</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-master</span><br><span class="line">spark-slave1</span><br><span class="line">spark-slave2</span><br></pre></td></tr></table></figure><p>（2）spark-defaults.conf暂时不变</p><p>（3）spark-env.sh中添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.8.0_161</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=spark-master</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/local/spark/spark-3.2.3-bin-hadoop3.2-scala2.13</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/local/hadoop/hadoop-3.2.3</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/local/hadoop/hadoop-3.2.3/etc/hadoop/</span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=/usr/local/hadoop/hadoop-3.2.3/etc/hadoop/</span><br></pre></td></tr></table></figure><p>4、/etc/profile中添加环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/local/spark/spark-3.2.3-bin-hadoop3.2-scala2.13</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><p>5、使配置生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h2 id="master配置同步到worker"><span class="post-title-index">4.2.</span><a href="#master配置同步到worker" class="headerlink" title="master配置同步到worker"></a> master配置同步到worker</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rsync -P -avz /usr/local/spark 192.168.56.102:/usr/local/</span><br><span class="line">rsync -P -avz /usr/local/spark 192.168.56.103:/usr/local/</span><br><span class="line">rsync -P -avz /etc/profile 192.168.56.102:/etc/profile</span><br><span class="line">rsync -P -avz /etc/profile 192.168.56.103:/etc/profile</span><br></pre></td></tr></table></figure><h2 id="运行Spark"><span class="post-title-index">4.3.</span><a href="#运行Spark" class="headerlink" title="运行Spark"></a> 运行Spark</h2><p>1、启动spark（在master节点上执行）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark/spark-3.2.3-bin-hadoop3.2-scala2.13/sbin</span><br><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><p>根据提示，依次输入两台worker节点的密码。（这里最好配置上免密登录）<br>这样，三个节点上的spark就都可以启动起来。</p><p>2、验证安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps -l</span><br></pre></td></tr></table></figure><p>master节点看到Master和Worker进程，worker节点看到Worker进程。</p><p>3、浏览器访问<br>浏览器访问 <a target="_blank" rel="noopener" href="http://192.168.56.101:8080/">http://192.168.56.101:8080</a><br>可以看到spark master信息。<br>浏览器访问 <a target="_blank" rel="noopener" href="http://192.168.56.102:8081/">http://192.168.56.102:8081</a><br>可以看到spark slave节点信息。</p><h1 id="测试使用Spark-on-Yarn"><span class="post-title-index">5.</span><a href="#测试使用Spark-on-Yarn" class="headerlink" title="测试使用Spark on Yarn"></a> 测试使用Spark on Yarn</h1><p>1、spark-submit提交任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn \</span><br><span class="line">  ./examples/jars/spark-examples_2.13-3.2.3.jar 1000</span><br></pre></td></tr></table></figure><p>3、浏览器查看进度<br>浏览器访问 <a target="_blank" rel="noopener" href="http://192.168.50.105:8088/">http://192.168.50.105:8088</a><br>在Yarn ResourceManager页面，可以查看到任务详情。</p><h1 id="vcore数量不对问题"><span class="post-title-index">6.</span><a href="#vcore数量不对问题" class="headerlink" title="vcore数量不对问题"></a> vcore数量不对问题</h1><h2 id="问题描述"><span class="post-title-index">6.1.</span><a href="#问题描述" class="headerlink" title="问题描述"></a> 问题描述</h2><p>理论上：<code>vcores使用数 = executor-cores * num-executors + 1</code><br>但是实际提交任务后，配置的executor-cores并没有起作用。</p><p>例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--num-executors 4 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.13-3.2.3.jar 10</span><br></pre></td></tr></table></figure><p>上面的提交，理论上应该使用<code>2 * 4 + 1 = 9</code>核，实际上从Yarn ResourceManager页面查看Allocated CPU VCores，只分配了5核。<br>也就是说executor-cores没有生效，使用了默认值1。</p><h2 id="解决办法"><span class="post-title-index">6.2.</span><a href="#解决办法" class="headerlink" title="解决办法"></a> 解决办法</h2><p>1、编辑 capacity-scheduler.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/hadoop/hadoop-3.2.3</span><br><span class="line">vim etc/hadoop/capacity-scheduler.xml</span><br></pre></td></tr></table></figure><p>如下修改：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.resource-calculator<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- &lt;value&gt;org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator&lt;/value&gt; --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    The ResourceCalculator implementation to be used to compare</span><br><span class="line">    Resources in the scheduler.</span><br><span class="line">    The default i.e. DefaultResourceCalculator only uses Memory while</span><br><span class="line">    DominantResourceCalculator uses dominant-resource to compare</span><br><span class="line">    multi-dimensional resources such as Memory, CPU etc.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>2、同步到slave节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rsync -P -avz /usr/local/hadoop/hadoop-3.2.3/etc/hadoop/ 192.168.56.102:/usr/local/hadoop/hadoop-3.2.3/etc/hadoop</span><br><span class="line">rsync -P -avz /usr/local/hadoop/hadoop-3.2.3/etc/hadoop/ 192.168.56.103:/usr/local/hadoop/hadoop-3.2.3/etc/hadoop</span><br></pre></td></tr></table></figure><p>3、重启yarn</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./sbin/stop-yarn.sh</span><br><span class="line">./sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>参考文档：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/yesecangqiong/p/10125333.html">spark on yarn提交后vcore数不对</a></p></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> 好好学习的郝</li><li class="post-copyright-link"> <strong>原文链接：</strong> <a href="https://www.voidking.com/dev-spark-on-yarn/" title="搭建Spark on Yarn集群">https://www.voidking.com/dev-spark-on-yarn/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本文采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议，转载请注明出处！源站会即时更新知识点并修正错误，欢迎访问~</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag"># 问题排查</a> <a href="/tags/java/" rel="tag"># java</a> <a href="/tags/hadoop/" rel="tag"># hadoop</a> <a href="/tags/spark/" rel="tag"># spark</a></div><div class="post-nav"><div class="post-nav-item"><a href="/dev-chatgpt-mirror-site/" rel="prev" title="部署ChatGPT镜像站"><i class="fa fa-chevron-left"></i> 部署ChatGPT镜像站</a></div><div class="post-nav-item"> <a href="/dev-helm-doc-extract/" rel="next" title="Helm官方文档摘录">Helm官方文档摘录<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="footer-ads" style="margin-top:10px"><ins class="adsbygoogle" style="display:block" data-ad-format="autorelaxed" data-ad-client="ca-pub-3284447971731414" data-ad-slot="9697986181"></ins><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3284447971731414" crossorigin="anonymous"></script><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-text">1. 前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85Java"><span class="nav-text">2. 安装Java</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85Hadoop"><span class="nav-text">3. 安装Hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BDHadoop%E5%AE%89%E8%A3%85%E5%8C%85"><span class="nav-text">3.1. 下载Hadoop安装包</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9Hadoop%E9%85%8D%E7%BD%AE"><span class="nav-text">3.2. 修改Hadoop配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E6%9C%BA%E6%B5%8B%E8%AF%95%E8%BF%90%E8%A1%8C"><span class="nav-text">3.3. 单机测试运行</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#master%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AEHDFS"><span class="nav-text">3.4. master节点配置HDFS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#master%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE%E9%85%8D%E7%BD%AEYarn"><span class="nav-text">3.5. master节点配置配置Yarn</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="nav-text">3.6. 配置环境变量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8B%B7%E8%B4%9D%E9%85%8D%E7%BD%AE%E5%88%B0slave%E8%8A%82%E7%82%B9"><span class="nav-text">3.7. 拷贝配置到slave节点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#master%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8HDFS"><span class="nav-text">3.8. master节点启动HDFS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#master%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8Yarn"><span class="nav-text">3.9. master节点启动Yarn</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85Spark"><span class="nav-text">4. 安装Spark</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#master%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE"><span class="nav-text">4.1. master节点配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#master%E9%85%8D%E7%BD%AE%E5%90%8C%E6%AD%A5%E5%88%B0worker"><span class="nav-text">4.2. master配置同步到worker</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%90%E8%A1%8CSpark"><span class="nav-text">4.3. 运行Spark</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E4%BD%BF%E7%94%A8Spark-on-Yarn"><span class="nav-text">5. 测试使用Spark on Yarn</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#vcore%E6%95%B0%E9%87%8F%E4%B8%8D%E5%AF%B9%E9%97%AE%E9%A2%98"><span class="nav-text">6. vcore数量不对问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0"><span class="nav-text">6.1. 问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="nav-text">6.2. 解决办法</span></a></li></ol></li></ol></div></div><div class="sidecar-ads" style="margin-top:10px"><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="好好学习的郝" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">好好学习的郝</p><div class="site-description" itemprop="description">一个计算机技术爱好者与学习者</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">655</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">29</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">242</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="mailto:voidking@qq.com" title="E-Mail → mailto:voidking@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://github.com/voidking" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;voidking" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="http://weibo.com/voidking" title="Weibo → http:&#x2F;&#x2F;weibo.com&#x2F;voidking" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/voidking" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;voidking" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i> Zhihu</a></span></div></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2014 – <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">好好学习的郝</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div><div class="footer-beian"> <a href="http://beian.miit.gov.cn/" target="_blank">苏ICP备14021030号</a>&nbsp;|&nbsp; <img src="/images/beian.png" alt=""> <a target="_blank" href="http://www.beian.gov.cn/">苏公网安备 32032202000223号</a></div></div></footer></div><script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/gitalk/1.7.2/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/gitalk/1.7.2/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '5a238b8c32b1e4dd2156',
      clientSecret: 'bfb5d518626f6fdc7da0351d1e0cd37ab75c6361',
      repo: 'gitalk-comments',
      owner: 'voidking',
      admin: ['voidking'],
      id: 'f6fcf07955e966d4a19cb49ebb1772d5',
      title: '搭建Spark on Yarn集群',
      body: '欢迎留言，互相交流学习~',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></body></html>